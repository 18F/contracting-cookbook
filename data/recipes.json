[
 {
  "title": "Agile Development",
  "description": "Although the practices of agile software development has many flavors (e.g., Scrum, SAFe, XP, Kanban), the core principles of agile software are described in the [Agile Manifesto](http://www.agilemanifesto.org/). From a deliverable standpoint, the two key principles are to prioritize: (1) \"Working software over comprehensive documentation\" and (2) \"Responding to change over following a plan.\" As such, when the government prepares a contract solicitation for agile software development, the deliverables should almost always be \"working software\" delivered on a quick and regular basis.\n\nOften, agile software development is contrasted with \"waterfall\" processes. Waterfall involves a phased approach to software development where requirements gathering, analysis, design, development, testing, and user acceptance are broken out into discrete phases. In other words, in waterfall, design should not occur before all requirements are gathered, and development should not occur until design is completed. \n\nUnlike waterfall, agile generally requires that *all* of these phases occur within shorter cycles, commonly referred to as \"sprints\". In other words, as development occurs, new requirements are gathered, new designs will emerge, etc. Similarly, testing will occur throughout the process.\n\nWithin the US government, the [TechFAR](https://github.com/WhiteHouse/playbook/blob/gh-pages/_includes/techfar-online.md) is the leading source on explaining how agile software development can be harmonized with the Federal Acquisition Regulations.\n",
  "ingredients": [
   "The TechFAR",
   "A clear product vision",
   "A willingness to let users determine the product's direction"
  ],
  "directions": [
   "Review the TechFAR",
   "Embrace the fact that agile software development requires flexibility in direction",
   "Ensure that your agency is prepared to manage short iterative, deployment of software."
  ],
  "snippets": "The contractor shall use agile management best practices (e.g., story-point estimation, velocity measurement) for estimating, planning, managing risk, and communicating status, to enable the effective management of the project team, along with user and product-owner expectations as to what will be done and by when. The process should also enable immediate course corrections when the team isn’t performing at the highest levels possible, or if project priorities shift; and maximize the likelihood of overall project success in terms of cost, schedule, quality, and value. The contractor and product owner shall define and adopt user-acceptance criteria for user stories.",
  "authors": [
   "vzvenyach"
  ],
  "tags": [
   "agile"
  ],
  "basename": "agile-development"
 },
 {
  "title": "An API-First Data Portal",
  "description": "As part of the [Digital Accountability and Transparency Act of 2014](http://www.gpo.gov/fdsys/pkg/PLAW-113publ101/html/PLAW-113publ101.htm), the Treasury Department was required to 'improve the quality of data submitted to USASpending.gov.' To do this, Treasury included *API-first language* in its solicitation for a new USASpending.gov. An API-first approach separates the presentation of data on USASpending.gov from the source of the data. But it also follows a key tenet of API design, namely that 'the #1 best way to understand and address the weaknesses in an API's design and implementation is to use it in a production system.\n\nThis approach has many architectural advantages. It promotes consistency in data publication. It ensures that future changes can be made to USASpending.gov with minimal expense when user needs change (i.e., reduces technical debt). It allows other government websites (such as internal data dashboards) or other web services to connect to the data directly from the source (i.e., improves extensibility). Finally, it should help improve performance and security by allowing the developers to optimize the API instead of optimizing both an API and a separate application.\n\nIn sum, an API-first approach is a great choice when you have data you plan to present on a website or portal. If nothing else, it will reduce headaches later when you decide to change your site, which--trust me--you will.\n",
  "ingredients": [
   "A read API",
   "A data portal website"
  ],
  "directions": [
   "If starting on a brand new application or service, build out a robust REST API in accordance with the agency's applicable API standards. Otherwise, the first thing to do will be to ensure that you can build out a new API on an existing service or build out the existing API to ensure that it can power the desired presentation interface. This will involve some judgment about which approach to take. But the goal is to ensure that a clean, robust API is available.",
   "Once you have an API, ensure that your presentation interface is built using the API. Expect that, as the presentation interface matures, there will be a need to optimize and refine the design of the API itself. This is a good, and natural result of an API-First approach.",
   "Make sure that the API is well documented, so that it is easier to make future changes to the presentation interface (or adoption of alternative presentations)."
  ],
  "snippets": "Finally, [agency] requires that the source of data used in the site be the outbound API.",
  "authors": [
   "vzvenyach"
  ],
  "tags": [
   "APIs",
   "data"
  ],
  "basename": "api-first"
 },
 {
  "title": "Automated Testing",
  "description": "The [US Digital Services playbook](https://playbook.cio.gov) recommends having automated tests for your code base. While manual testing of code can be a good process, automated tests ensure greater code stability.\n\nAcceptance test are a type of automated test that encapsulate a feature from the end-user's point of view. For example, a web application will have automated acceptance tests for user login. Acceptance tests are executed in a real browser that is coded to execute user input. On the other end of the scale are unit tests which test detailed input, output and side-effects for smallest, most isolated modules of code. In between these extremes are functional or integration tests which test the interactions between more than one unit of code. Naming of test types is inconsistent across the industry, and sometimes you will hear 'acceptance' and 'integration' tests used interchangeably.\n\nWriting focused tests, that include detailed descriptions, provides the best possible form of documentation for communication between developers. Acceptance tests can be described in a English-like language called [Gherkin](https://github.com/cucumber/cucumber/wiki/Gherkin). Implementation for Gherkin is available in most modern programming languages. As a tool, Gherkin, extends the documentation role of automated tests, making them accessible to product managers and other stakeholders.\n\nRunning tests locally is a great tool for engineers, but the entire project benefits when tests are run after each commit to the master repository. Continuous integration servers provide this kind of project level safety net.\n\nAlmost every programming language has freely available, open-source testing frameworks. In addition, there are code coverage tools that calculate how well the tests cover the code.\n",
  "ingredients": [
   "Testable code",
   "A testing framework for your language",
   "Testing coverage tool (optional)",
   "Continuous testing integration server (optional)",
   "Patience and faith in the testing process"
  ],
  "directions": [
   "If starting a new feature, it is best to start with a failing acceptance test. That insures that the big-picture intent is covered by the new code. Next figure out what code you will have to write to make the acceptance test pass. A good practice is to write unit and functional tests first, before writing the code. This practice is called Test Driven Development, or TDD for short. Writing tests first forces you to think about the interface for your code rather than the implementation. Be careful not to write tests that tie you to a particular implementation. The unit tests should treat each chunk of the code under test as a black box.",
   "When starting a new bug, see if you can reproduce the defect in a test before investigating the source of the bug. Having that kind of test will ensure that you know when you have solved the problem. Sometimes the generated test is an acceptance test and when the root cause of the bug is found, the acceptance test can be transformed into a unit test. Don't be afraid to delete and transform test. The goal is to have covered code, not to maximize the number of tests.",
   "Acceptance tests tend to take a long time to run, while unit tests should happen in milliseconds. Write many unit tests, some functional tests and few acceptance tests. Test suites that invert these ratios become dauntingly slow, frustrating engineers.",
   "When starting testing on a code base that has no tests or doesn't have enough test, be practical. It can be an overwhelming exercise in boiling the ocean to try to get everything under test. Instead focus on putting tests around anything that is changing. As features, bugs and refactorings become necessary, put all the code that you touch under test. Test coverage can be determined with freely available test coverage tools, available in most languages.",
   "To ensure that your tests have the maximum impact for your project, work with a continuous integration server. There are hosted services that provide this server for your tests and can be connected to your code repository for automatic runs. One example is [Travis CI](https://travis-ci.org/). Travis CI is also an open source project and can be hosted on your private servers."
  ],
  "snippets": "The contractor shall use test-driven development practices (TDD). TDD helps ensure increased protection from defects, better code quality, and tests serving as substitutes for other forms of documentation. There shall be sufficient test coverage to ensure that new user stories can be added without doubts about whether earlier features are affected. Testing shall include automated unit tests, integration tests, functional testing, and regression testing.",
  "authors": [
   "baccigalupi",
   "vzvenyach"
  ],
  "tags": [
   "Unit Testing",
   "Integration Testing"
  ],
  "basename": "automated-testing"
 },
 {
  "title": "Continuous Deployment",
  "description": "[Play 10](https://playbook.cio.gov/#play10) of the US Digital Services Playbook is to \"[a]utomate testing and deployments.\" As discussed in \"Automated Testing,\" it is imperative to ensure continuous integration. But equally important is ensuring continous deployment.\n\nAs a practical matter, implementing continuous deployment is more of a workflow pattern than a specific toolset. The general premise of continuous deployment is that an application should be able to automatically deploy--regularly throughout the development process--into multiple, nearly identical environments. The advantage of continuous deployment is that a developer should be able to make application-level changes without worrying about the overhead of deploying those changes into various environments (see \"Hosting Environments\").\n\nThere are many different ways to achieve continuous deployment (e.g., build packs, container-based deployments, etc.), but the central requirement is to ensure no-downtime, automated deployments into a given host environment based on changes to the application's codebase.\n",
  "ingredients": [
   "Host environments",
   "Application-level source control",
   "Automated testing"
  ],
  "directions": [
   null
  ],
  "snippets": "The contractor shall use software configuration/deployment automation tools (e.g., Capistrano, Chef, Docker), so that developers can deploy code changes to the target environment (e.g., staging) with the issuance of a single command.",
  "authors": [
   "vzvenyach"
  ],
  "tags": [
   "continuous deployment"
  ],
  "basename": "continuous-deployment"
 },
 {
  "title": "Hosting Environments",
  "description": "In almost every modern project, a software developer will develop an application on his or her computer (\"locally\") and will deploy the application into one or more \"hosting environments.\" Normally, these environments allow the development team to accomplish certain goals, such as:\n\n* Development: to allow the team to build the application;\n* Testing: to make sure that the application performs as it is expected;\n* Staging: to make sure that the application is ready for production; and\n* Production: to allow end-users to access the applicaton.\n\nHistorically, each environment would be separately \"provisioned\" and configured manually based on checklists or \"scripts.\" This pattern is still very common. However, the modern pattern is to allow for \"continuous deployment\", to automate the provisioning of these environments and using configuration-management tools and practices to ensure parity between the different environments. Although there are many different tools used to achieve continuous deployment, the basic principle is that any changes needed made to the hosting environments should be automatically and reproducibly deployed across the hosting environments.\n\nOne good way to ensure that your application's codebase is ready for continuous deployment is to specify that the application be a [12 Factor App](http://12factor.net/). In practice, this will require some effort on the part of your development team, but it will help ensure long-term sustainability of the application.  \n",
  "ingredients": [
   "Multiple servers",
   "Configuration management tools (e.g., Puppet, Chef, Ansible)"
  ],
  "directions": [
   "Determine whether the offeror will deploy to infrastructure hosted by the government or on infrastructure hosted by the offeror.",
   "To the extent that the government will ultimately host, ensure that the government's infrastructure-service provider can provide your development team with the ability to specify and use configuration management tools."
  ],
  "snippets": "The offeror shall ensure that the application meet the '12-factor application' methodology to ensure automated, reproducible deployments to multiple environments. The offeror shall deploy development, staging, and production environments within the Amazon Web Service Cloud provided by [agency].",
  "tags": [
   "continuous deployment",
   "agile",
   "hosting"
  ],
  "basename": "hosting-environments"
 },
 {
  "title": "Migrating to the Cloud",
  "description": "According to the [NIST definition of \"cloud\"](http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf): \n\n> Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.\n\nTraditionally from the government's perspective, the most attractive feature of the cloud is that it is \"on-demand.\" If you have an application that requires a new server, the cloud should make it trivial to simply \"spin up\" a new server. Similarly, if you have an existing server, but you need to duplicate it to respond to significant \"load\" on the server, the cloud should make it trivial to spin up a copy of that server. This dynamic quality of the cloud enables entirely different software development/deployment patterns from those that existed before the cloud.\n\nThere are 3 common \"service models\" for cloud products: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). Depending on the agency's needs, an agency will need to buy multiple cloud products within each service model. For the most part, IaaS and SaaS (less so for PaaS) are largely \"commodity\" offerings in the sense that costs and services are largely fixed by market conditions. Nevertheless, defining an agency's requirements is often quite difficult, because the cloud fundamentally affects the deployment patterns used by the program office. As such, 18F generally advises agencies to adopt an [phased, incremental approach to \"cloud migration\"](https://18f.gsa.gov/2015/06/22/avoiding-cloudfall/), focusing on buying smaller increments of infrastucture early on and expanding based on the agency's deployment needs.\n",
  "ingredients": [
   "[FedRAMP](https://www.fedramp.gov/)"
  ],
  "directions": [
   "Before buying cloud services, familiarize yourself with FedRAMP, including systems that have received a FedRAMP approved ATO (Authority to Operate).",
   "Develop an agency-specific framework for a phased approach to cloud migration. In doing so, take advantage of existing contract vehicles and micropurchase thresholds for discovery phases."
  ],
  "snippets": "",
  "authors": [
   "vzvenyach"
  ],
  "tags": [
   "cloud"
  ],
  "basename": "migrating-to-the-cloud"
 },
 {
  "title": "Modular procurement and modular design",
  "description": "The Clinger-Cohen Act, as amended by the Federal Information Technology Acquisition Reform Act, and [OMB guidance](https://www.whitehouse.gov/sites/default/files/omb/memoranda/2015/m-15-14.pdf),require that CIOs use \"incremental or modular contracting.\" The [FAR](https://www.acquisition.gov/sites/default/files/current/far/html/Subpart%2039_1.html) defines modular contracting as \"us[ing] of one or more contracts to acquire information technology systems in successive, interoperable increments\" and [GAO](http://www.gao.gov/products/GAO-14-361) has  In fact, pursuant to [FAR 39.103](https://www.acquisition.gov/sites/default/files/current/far/html/Subpart%2039_1.html#wp1096828), agencies are advised \"to the maximum extent practicable, [to] use modular contracting to acquire major systems [and] non-major systems of information technology.\"\n\nFortunately, modern software architecture patterns naturally promote the use of incremental and modular procurement. Specifically, best practices in modern software development generally ensure [\"loose coupling\"](https://en.wikipedia.org/wiki/Loose_coupling) and [\"separation of concerns\"](https://en.wikipedia.org/wiki/Separation_of_concerns), both of which ultimately hinge on the idea that individual components or subsystems can (and should) be built separately. In some instances, the use of a \"service-oriented architecture\" or \"microservice architecture\" can be used to implement modularity on more complicated systems.\n\nAccordingly, when developing a new IT system, agencies can take advantage of modular contracting by using multiple contracts (whether successive or in parallel) for the development of different components or modules. Moreover, even when a single contract is used, agencies should be ensuring that vendors are using modular design practices to ensure the system's long-term sustainability.\n",
  "ingredients": [
   "A technical architecture diagram (does not need to be too detailed)",
   "An acquisition strategy (e.g., a multiple-award BPA) that ensures modular contracting"
  ],
  "directions": [
   "First, ensure that there is an acquisition strategy that allows for modular contracting.",
   "Using the technical architecture diagram, determine which software modules or components should be built separately.",
   "Use multiple orders to build out the modules than can be built separately.",
   "Even within a single order, ensure that the vendor's technical approach takes advantage of modular design."
  ],
  "snippets": "(As part of the evaluation criteria) The offeror's technical approach shall describe how the vendor will ensure loose coupling and separation of concerns, where appropriate.",
  "tags": [
   "modular design",
   "FITARA"
  ],
  "basename": "modular-design"
 },
 {
  "title": "Open Source Software",
  "description": "\"Both the [US Digital Services playbook](https://playbook.cio.gov) and the [18F open-source policy](https://github.com/18F/open-source-policy/blob/master/policy.md) strongly suggest the use and creation of open-source software in the creation of digital services. Specifically, the 18F policy provides that the 'default position' should be to 'Use Free and Open Source Software (FOSS), which is software that does not charge users a purchase or licensing fee for modifying or redistributing the source code, in our projects and contribute back to the open source community.' Based on this default position, vendors who are developing software for the government should also use and create open-source software. In a recent solicitation from the Department of Defense, the government included a requirement that all deliverables by made available to the public as open-source software, and more importantly, affords the government the authority to select the applicable license.\n\nThere are permutations of the language, but 18F's favorite recipe involves requiring not only open-source software, but further requires that all software delivered be dedicated to the public domain under a Creative Commons Zero (CC0) license.\"\n",
  "ingredients": [
   "A requirement for new software deliverables",
   "One or more publicly available source-code repositories",
   "A preferred open-source license (18F uses CC0)"
  ],
  "directions": [
   "Pre-heat.",
   "Do some market research to determine whether or not there are open-source solutions available *before* you buy or build. Using pre-existing open-source libraries is a best practice, it's delightful to improve or support an existing open-source library.",
   "Make sure that your vendors understand, whether during an RFI or industry day, that open-source software is important for your project.",
   "Include in your solicitation a requirement for open-source software. If you have a preferred license, specify it in the solicitation. If not, give yourself the flexibility to choose a license at a later point.",
   "Make sure that your contractor holds to the open-source requirement, and require the contractor to provide all licenses for software dependencies as part of the deliverables."
  ],
  "snippets": "The Contractor shall develop all source code that should become property of the Government, for software deliverables which shall be made available by the contractor to the public as open source software, under the terms and conditions of a license determined or approved by the Government.",
  "tags": [
   "Open Source"
  ],
  "basename": "open-source"
 },
 {
  "title": "Source Control",
  "description": "The [US Digital Services playbook](https://playbook.cio.gov) recommends using a 'source code version control system' while developing digital services. Source control, also known as version control, is a tool that tracks history of changes made to code base. History is recorded via a series of commits. Source control systems usually allow rolling back to previous states, which can be essential when bad code is commited to the code base accidentally. More information on the benefits and features of source control systems are described on [Wikipedia](https://en.wikipedia.org/wiki/Version_control).\n\nIn addition to providing rollback capabilities, source control is important for code bases with multiple contributors. Each commit encapsulates the intent of the change to the code base, and when two contributors edit the same file or even the same line, the version control system will recognize the conflict, preventing lost work.\n\nSource control tools are also heavily used in modern software engineering as an integration point for other important tools. A commit to the master repository can trigger automated test builds, static analysis or even a deploy.\n",
  "ingredients": [
   "A code base",
   "A source control tool. Recommended source control tools are [git](https://git-scm.com) and [mercurial](https://mercurial.selenic.com). Both are [distributed](https://en.wikipedia.org/wiki/Distributed_version_control) source control systems.",
   "A central repository to act as the source of truth for the code base"
  ],
  "directions": [
   "Preheat - Prepare your repository for source control: Some binary files are not appropriate for source control. In the case of large binary files, the file changes cannot be comprehended easily by mere mortals, so recording the differences in the files looses its meaning. Also, binary files are often very large, requiring massive transfer bandwidth. Binaries are appropriate when an integral part of the history. For example, many images are relevant and changing resources in a web application. Omiting them from the repository would cause more pain than gain. Careful thought should go into what should be in and out of the repository. Source control tools allow ignoring files.",
   "Learn your source control tool. Both recommended tools are open source with great tutorials and community support. You can get rapid benefits without having to fully know your tool.",
   "Create a central repository to act and the source of truth. 18F uses git as a tool and hosts repositories via [Github](https://github.com). Mercurial repositories can be likewise hosted on the service [Bitbucket](https://bitbucket.org/). It is possible to roll your own repository host on a server, but it is likely that the laws governing your digital content allow source control hosting via a cloud service."
  ],
  "authors": [
   "baccigalupi",
   "vzvenyach"
  ],
  "snippets": "",
  "tags": [
   "Source Control"
  ],
  "basename": "source-control"
 },
 {
  "title": "User-Centered Design",
  "description": "One of the core principles of 18F's work is [user-centered design](http://www.usability.gov/what-and-why/user-centered-design.html). Although the concept of user-centered design is deceptively simple--software should be built to meet end users' goals--the practice requires substantial effort and expertise. Within government, it is all too common for program offices (and contracting officers) to make assumptions about how users will interact with a particular piece of software and to bake those assumptions into \"requirements.\" The unfortunate reality is that users rarely interact with systems precisely as we predict they might, which is why so much federal IT suffers from usability problems.\n\nBy embedding user-centered-design principles in a procurement, an agency can take steps toward ensuring that the end-product is more useful. At a minimum, before development commences, the government should ensure that the team has conducted sufficient [discovery through one or more methods](https://methods.18f.gov) to help determine the goals, needs, opportunities, and constraints of users relying on the systems. Ideally, user-centered design should be budgeted-for and integrated in the product development through the duration of the product's development.\n",
  "ingredients": [
   "Users. (Probably more users than you're used to.)",
   "A willingness to emphathize with your users.",
   "[Research Methods](https://methods.18f.gov/)"
  ],
  "directions": [
   "When developing your solicitation, determine whether you're doing enough to ensure that user research will inform the product development. This often means that you'll need to have key personnel who are skilled at conducting user research and usability testing.",
   "Include user-centered design into your technical requirements and include user-research methods as part of your evaluation criteria.",
   "Ensure that your product's designers are coordinating closely with developers and that the product's design is grounded in user research."
  ],
  "snippets": "Offeror shall employ user‐centered design principles and integrate continuous user‐research and usability testing into all aspects of the system.  ",
  "authors": [
   "vzvenyach"
  ],
  "tags": [
   "Open Source"
  ],
  "basename": "user-centered-design"
 }
]